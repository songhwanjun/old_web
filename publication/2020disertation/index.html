<!DOCTYPE html><html lang="en-us" >

<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  
  
  
    <meta name="generator" content="Wowchemy 5.3.0 for Hugo" />
  

  

  
  
  
  
  
    
    
    
  
  

  <meta name="author" content="Hwanjun Song" />

  
  
  
    
  
  <meta name="description" content="Deep learning has achieved remarkable success in numerous domains with help from large amounts of big data. However, the quality of data labels is a concern because of the lack of high-quality labels in many real-world scenarios. In the presence of noisy labels, the generalization performance of deep neural networks drastically falls down owing to their high capacity to overfit any noise labels. This overfitting issue still remains even with various conventional regularization techniques, such as dropout and batch normalization. Therefore, learning from noisy labels (robust training) has recently become one of the most active research topics in the machine learning community. In the first part, we provide the problem statement for supervised learning with noisy labels, followed by a thorough survey on the advance in recent deep learning techniques for overcoming noisy labels; we surveyed recent studies by recursively tracking relevant bibliographies in papers published at premier research conferences. Throughout this survey, we note that the main research effort has been made to answer the two following questions - (1) how to minimize the negative in uence of false-labeled samples by adjusting their loss values? and (2) how to identify true-labeled samples from noisy data?, both of which have been well-explored respectively by the two research directions, namely, loss adjustment and sample selection. In the second part, we mainly focus on understanding the pros and cons of the aforementioned research directions and, subsequently, propose a hybrid learning approach called SELFIE that takes advantage of both loss adjustment and sample selection. For the hybrid approach, a new concept of a refurbishable sample is introduced to classify the sample whose loss can be correctly adjusted with high precision. The loss of refurbishable samples is adjusted First and then combined with that of the samples chosen by a representative sample selection criterion called small-loss trick. To validate the superiority of SELFIE, we conducted extensive experimentation using both real-world or synthetic noisy datasets. The results empirically verify that SELFIE significantly outperforms state-of-the-art methods in test error by up to 10.5 percentage point. In the third part, we take a closer look at the small-loss trick adopted by SELFIE for sample selection. We argue that the trick misclassifies many false-labeled samples as clean samples in realistic noise. Hence, we present a new sample selection method called Prestopping, which derives a collection of true-labeled samples by using the early stopping mechanism. Prestopping obtains an initial safe set by stopping its learning process before the network begins to rapidly memorize false-labeled samples and, subsequently, resumes training to improve the quality and quantity of the set gradually. Compared with state-of-the-art methods including SELFIE, Prestopping further improves the test error by up to 18.1 percentage point on four real-world or synthetic noisy datasets. The main technical challenge in Prestopping is determining the best stop point for its phase transition (we call it a best transition point). In Prestopping, a clean validation set or a known true noise rate is used for supervision, but they are usually hard to acquire in practice. In the last part, we introduce a novel self-transitional learning approach called MORPH, which automatically switches its learning phase at the best transition point without any supervision. Extensive experiments using five benchmark datasets demonstrate that only MORPH succeeds to construct a collection of almost true-labeled samples in a wide range of noise types. We leave the incorporation of SELFIE with MORPH as future work." />

  
  <link rel="alternate" hreflang="en-us" href="https://example.com/publication/2020disertation/" />

  









  




  
  
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin />
  

  
  
  
    <meta name="theme-color" content="#1565c0" />
  

  
  

  

  <link rel="stylesheet" href="/css/vendor-bundle.min.f1ecf783c14edc00c9320c205831ad8e.css" media="print" onload="this.media='all'">

  
  
  
    
    
      <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.9.0/css/academicons.min.css" integrity="sha512-W4yqoT1+8NLkinBLBZko+dFB2ZbHsYLDdr50VElllRcNt2Q4/GSs6u71UHKxB7S6JEMCp5Ve4xjh3eGQl/HRvg==" crossorigin="anonymous" media="print" onload="this.media='all'">
    

    
    
    
    
      
      
    
    
    

    
    
    
      
    
    
      
      
        
          <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.2.0/styles/github.min.css" crossorigin="anonymous" title="hl-light" media="print" onload="this.media='all'">
          <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.2.0/styles/dracula.min.css" crossorigin="anonymous" title="hl-dark" media="print" onload="this.media='all'" disabled>
        
      
    

    
    
    

    

    
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      
        
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
  

  
  
  
    
      
      <link rel="preload" as="style" href="https://fonts.googleapis.com/css2?family=Montserrat:wght@400;700&family=Roboto+Mono&family=Roboto:wght@400;700&display=swap">
      <link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Montserrat:wght@400;700&family=Roboto+Mono&family=Roboto:wght@400;700&display=swap" media="print" onload="this.media='all'">
    
  

  
  
  
  
  
  <link rel="stylesheet" href="/css/wowchemy.481af39c39ffd87b2d14f39943e7c723.css" />

  



  

  

  




  
  
  

  

  
    <link rel="manifest" href="/manifest.webmanifest" />
  

  <link rel="icon" type="image/png" href="/media/icon_hu0b7a4cb9992c9ac0e91bd28ffd38dd00_9727_32x32_fill_lanczos_center_3.png" />
  <link rel="apple-touch-icon" type="image/png" href="/media/icon_hu0b7a4cb9992c9ac0e91bd28ffd38dd00_9727_180x180_fill_lanczos_center_3.png" />

  <link rel="canonical" href="https://example.com/publication/2020disertation/" />

  
  
  
  
  
  
  
  
    
  
  
  <meta property="twitter:card" content="summary_large_image" />
  
  <meta property="og:site_name" content="hwanjun.me" />
  <meta property="og:url" content="https://example.com/publication/2020disertation/" />
  <meta property="og:title" content="Robust Learning under Label Noise with Deep Neural Networks (PhD Dissertation) | hwanjun.me" />
  <meta property="og:description" content="Deep learning has achieved remarkable success in numerous domains with help from large amounts of big data. However, the quality of data labels is a concern because of the lack of high-quality labels in many real-world scenarios. In the presence of noisy labels, the generalization performance of deep neural networks drastically falls down owing to their high capacity to overfit any noise labels. This overfitting issue still remains even with various conventional regularization techniques, such as dropout and batch normalization. Therefore, learning from noisy labels (robust training) has recently become one of the most active research topics in the machine learning community. In the first part, we provide the problem statement for supervised learning with noisy labels, followed by a thorough survey on the advance in recent deep learning techniques for overcoming noisy labels; we surveyed recent studies by recursively tracking relevant bibliographies in papers published at premier research conferences. Throughout this survey, we note that the main research effort has been made to answer the two following questions - (1) how to minimize the negative in uence of false-labeled samples by adjusting their loss values? and (2) how to identify true-labeled samples from noisy data?, both of which have been well-explored respectively by the two research directions, namely, loss adjustment and sample selection. In the second part, we mainly focus on understanding the pros and cons of the aforementioned research directions and, subsequently, propose a hybrid learning approach called SELFIE that takes advantage of both loss adjustment and sample selection. For the hybrid approach, a new concept of a refurbishable sample is introduced to classify the sample whose loss can be correctly adjusted with high precision. The loss of refurbishable samples is adjusted First and then combined with that of the samples chosen by a representative sample selection criterion called small-loss trick. To validate the superiority of SELFIE, we conducted extensive experimentation using both real-world or synthetic noisy datasets. The results empirically verify that SELFIE significantly outperforms state-of-the-art methods in test error by up to 10.5 percentage point. In the third part, we take a closer look at the small-loss trick adopted by SELFIE for sample selection. We argue that the trick misclassifies many false-labeled samples as clean samples in realistic noise. Hence, we present a new sample selection method called Prestopping, which derives a collection of true-labeled samples by using the early stopping mechanism. Prestopping obtains an initial safe set by stopping its learning process before the network begins to rapidly memorize false-labeled samples and, subsequently, resumes training to improve the quality and quantity of the set gradually. Compared with state-of-the-art methods including SELFIE, Prestopping further improves the test error by up to 18.1 percentage point on four real-world or synthetic noisy datasets. The main technical challenge in Prestopping is determining the best stop point for its phase transition (we call it a best transition point). In Prestopping, a clean validation set or a known true noise rate is used for supervision, but they are usually hard to acquire in practice. In the last part, we introduce a novel self-transitional learning approach called MORPH, which automatically switches its learning phase at the best transition point without any supervision. Extensive experiments using five benchmark datasets demonstrate that only MORPH succeeds to construct a collection of almost true-labeled samples in a wide range of noise types. We leave the incorporation of SELFIE with MORPH as future work." /><meta property="og:image" content="https://example.com/publication/2020disertation/featured.gif" />
    <meta property="twitter:image" content="https://example.com/publication/2020disertation/featured.gif" /><meta property="og:locale" content="en-us" />
  
    
      <meta
        property="article:published_time"
        content="2020-12-16T00:00:00&#43;00:00"
      />
    
    <meta property="article:modified_time" content="2021-02-28T00:00:00&#43;00:00">
  

  


    









<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "Article",
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://example.com/publication/2020disertation/"
  },
  "headline": "Robust Learning under Label Noise with Deep Neural Networks (PhD Dissertation)",
  
  "image": [
    "https://example.com/publication/2020disertation/featured.gif"
  ],
  
  "datePublished": "2020-12-16T00:00:00Z",
  "dateModified": "2021-02-28T00:00:00Z",
  
  "author": {
    "@type": "Person",
    "name": "Hwanjun Song"
  },
  
  "publisher": {
    "@type": "Organization",
    "name": "hwanjun.me",
    "logo": {
      "@type": "ImageObject",
      "url": "https://example.com/media/icon_hu0b7a4cb9992c9ac0e91bd28ffd38dd00_9727_192x192_fill_lanczos_center_3.png"
    }
  },
  "description": "Deep learning has achieved remarkable success in numerous domains with help from large amounts of big data. However, the quality of data labels is a concern because of the lack of high-quality labels in many real-world scenarios. In the presence of noisy labels, the generalization performance of deep neural networks drastically falls down owing to their high capacity to overfit any noise labels. This overfitting issue still remains even with various conventional regularization techniques, such as dropout and batch normalization. Therefore, learning from noisy labels (robust training) has recently become one of the most active research topics in the machine learning community. In the first part, we provide the problem statement for supervised learning with noisy labels, followed by a thorough survey on the advance in recent deep learning techniques for overcoming noisy labels; we surveyed recent studies by recursively tracking relevant bibliographies in papers published at premier research conferences. Throughout this survey, we note that the main research effort has been made to answer the two following questions - (1) how to minimize the negative in uence of false-labeled samples by adjusting their loss values? and (2) how to identify true-labeled samples from noisy data?, both of which have been well-explored respectively by the two research directions, namely, loss adjustment and sample selection. In the second part, we mainly focus on understanding the pros and cons of the aforementioned research directions and, subsequently, propose a hybrid learning approach called SELFIE that takes advantage of both loss adjustment and sample selection. For the hybrid approach, a new concept of a refurbishable sample is introduced to classify the sample whose loss can be correctly adjusted with high precision. The loss of refurbishable samples is adjusted First and then combined with that of the samples chosen by a representative sample selection criterion called small-loss trick. To validate the superiority of SELFIE, we conducted extensive experimentation using both real-world or synthetic noisy datasets. The results empirically verify that SELFIE significantly outperforms state-of-the-art methods in test error by up to 10.5 percentage point. In the third part, we take a closer look at the small-loss trick adopted by SELFIE for sample selection. We argue that the trick misclassifies many false-labeled samples as clean samples in realistic noise. Hence, we present a new sample selection method called Prestopping, which derives a collection of true-labeled samples by using the early stopping mechanism. Prestopping obtains an initial safe set by stopping its learning process before the network begins to rapidly memorize false-labeled samples and, subsequently, resumes training to improve the quality and quantity of the set gradually. Compared with state-of-the-art methods including SELFIE, Prestopping further improves the test error by up to 18.1 percentage point on four real-world or synthetic noisy datasets. The main technical challenge in Prestopping is determining the best stop point for its phase transition (we call it a best transition point). In Prestopping, a clean validation set or a known true noise rate is used for supervision, but they are usually hard to acquire in practice. In the last part, we introduce a novel self-transitional learning approach called MORPH, which automatically switches its learning phase at the best transition point without any supervision. Extensive experiments using five benchmark datasets demonstrate that only MORPH succeeds to construct a collection of almost true-labeled samples in a wide range of noise types. We leave the incorporation of SELFIE with MORPH as future work."
}
</script>

  

  

  

  





  <title>Robust Learning under Label Noise with Deep Neural Networks (PhD Dissertation) | hwanjun.me</title>
</head>


<body id="top" data-spy="scroll" data-offset="70" data-target="#TableOfContents" class="page-wrapper   " data-wc-page-id="c5c6c85d03e6c975d13b23cc2c949b4e" >

  
  
  
  
  
  
  
  
  
  <script src="/js/wowchemy-init.min.8988fb2a4bba758785868cfcb5244555.js"></script>

  


<aside class="search-modal" id="search">
  <div class="container">
    <section class="search-header">

      <div class="row no-gutters justify-content-between mb-3">
        <div class="col-6">
          <h1>Search</h1>
        </div>
        <div class="col-6 col-search-close">
          <a class="js-search" href="#" aria-label="Close"><i class="fas fa-times-circle text-muted" aria-hidden="true"></i></a>
        </div>
      </div>

      <div id="search-box">
        
        <input name="q" id="search-query" placeholder="Search..." autocapitalize="off"
        autocomplete="off" autocorrect="off" spellcheck="false" type="search" class="form-control"
        aria-label="Search...">
        
      </div>

      
      

      

    </section>
    <section class="section-search-results">

      <div id="search-hits">
        
      </div>

    </section>
  </div>
</aside>



  <div class="page-header">
    












<nav class="navbar navbar-expand-lg navbar-light compensate-for-scrollbar" id="navbar-main">
  <div class="container-xl">

    
    <div class="d-none d-lg-inline-flex">
      <a class="navbar-brand" href="/">hwanjun.me</a>
    </div>
    

    
    <button type="button" class="navbar-toggler" data-toggle="collapse"
            data-target="#navbar-content" aria-controls="navbar-content" aria-expanded="false" aria-label="Toggle navigation">
    <span><i class="fas fa-bars"></i></span>
    </button>
    

    
    <div class="navbar-brand-mobile-wrapper d-inline-flex d-lg-none">
      <a class="navbar-brand" href="/">hwanjun.me</a>
    </div>
    

    
    
    <div class="navbar-collapse main-menu-item collapse justify-content-start" id="navbar-content">

      
      <ul class="navbar-nav d-md-inline-flex">
        

        

        
        
        
          
        

        

        
        
        
        

        
          
            
            
          
          
            
            
            
              
            
            
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#about"><span>Home</span></a>
        </li>

        
        

        

        
        
        
          
        

        

        
        
        
        

        
          
            
            
          
          
            
            
            
              
            
            
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#accomplishments"><span>Accomplishments</span></a>
        </li>

        
        

        

        
        
        
          
        

        

        
        
        
        

        
          
            
            
          
          
            
            
            
              
            
            
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#publications"><span>Publications</span></a>
        </li>

        
        

      

        
      </ul>
    </div>

    <ul class="nav-icons navbar-nav flex-row ml-auto d-flex pl-md-2">

      
      
        
      

      
      
      <li class="nav-item">
        <a class="nav-link js-search" href="#" aria-label="Search"><i class="fas fa-search" aria-hidden="true"></i></a>
      </li>
      

      
      
      <li class="nav-item dropdown theme-dropdown">
        <a href="#" class="nav-link" data-toggle="dropdown" aria-haspopup="true" aria-label="Display preferences">
          <i class="fas fa-moon" aria-hidden="true"></i>
        </a>
        <div class="dropdown-menu">
          <a href="#" class="dropdown-item js-set-theme-light">
            <span>Light</span>
          </a>
          <a href="#" class="dropdown-item js-set-theme-dark">
            <span>Dark</span>
          </a>
          <a href="#" class="dropdown-item js-set-theme-auto">
            <span>Automatic</span>
          </a>
        </div>
      </li>
      

      
      

    </ul>

  </div>
</nav>


  </div>

  <div class="page-body">
    








<div class="pub">

  




















  
  


<div class="article-container pt-3">
  <h1>Robust Learning under Label Noise with Deep Neural Networks (PhD Dissertation)</h1>

  

  


<div class="article-metadata">

  
  
  
  
  <div>
    

  <span >
      Hwanjun Song</span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    February 2021
  </span>
  

  

  

  
  
  
  
  
  

  
  

</div>

  




<div class="btn-links mb-3">
  
  








  
    
  



<a class="btn btn-outline-primary btn-page-header" href="https://github.com/songhwanjun/songhwanjun.github.io/raw/master/files/PhD_Dissertation.pdf" target="_blank" rel="noopener">
  PDF
</a>



<a href="#" class="btn btn-outline-primary btn-page-header js-cite-modal"
        data-filename="/publication/2020disertation/cite.bib">
  Cite
</a>















</div>


</div>


<div class="article-header article-container featured-image-wrapper mt-4 mb-4" style="max-width: 691px; max-height: 581px;">
  <div style="position: relative">
    <img src="/publication/2020disertation/featured.gif" alt="" class="featured-image">
    
  </div>
</div>



  <div class="article-container">

    
    <h3>Abstract</h3>
    <p class="pub-abstract">Deep learning has achieved remarkable success in numerous domains with help from large amounts of big data. However, the quality of data labels is a concern because of the lack of high-quality labels in many real-world scenarios. In the presence of noisy labels, the generalization performance of deep neural networks drastically falls down owing to their high capacity to overfit any noise labels. This overfitting issue still remains even with various conventional regularization techniques, such as dropout and batch normalization. Therefore, learning from noisy labels (robust training) has recently become one of the most active research topics in the machine learning community. In the first part, we provide the problem statement for supervised learning with noisy labels, followed by a thorough survey on the advance in recent deep learning techniques for overcoming noisy labels; we surveyed recent studies by recursively tracking relevant bibliographies in papers published at premier research conferences. Throughout this survey, we note that the main research effort has been made to answer the two following questions - (1) how to minimize the negative in uence of false-labeled samples by adjusting their loss values? and (2) how to identify true-labeled samples from noisy data?, both of which have been well-explored respectively by the two research directions, namely, loss adjustment and sample selection. In the second part, we mainly focus on understanding the pros and cons of the aforementioned research directions and, subsequently, propose a hybrid learning approach called SELFIE that takes advantage of both loss adjustment and sample selection. For the hybrid approach, a new concept of a refurbishable sample is introduced to classify the sample whose loss can be correctly adjusted with high precision. The loss of refurbishable samples is adjusted First and then combined with that of the samples chosen by a representative sample selection criterion called small-loss trick. To validate the superiority of SELFIE, we conducted extensive experimentation using both real-world or synthetic noisy datasets. The results empirically verify that SELFIE significantly outperforms state-of-the-art methods in test error by up to 10.5 percentage point. In the third part, we take a closer look at the small-loss trick adopted by SELFIE for sample selection. We argue that the trick misclassifies many false-labeled samples as clean samples in realistic noise. Hence, we present a new sample selection method called Prestopping, which derives a collection of true-labeled samples by using the early stopping mechanism. Prestopping obtains an initial safe set by stopping its learning process before the network begins to rapidly memorize false-labeled samples and, subsequently, resumes training to improve the quality and quantity of the set gradually. Compared with state-of-the-art methods including SELFIE, Prestopping further improves the test error by up to 18.1 percentage point on four real-world or synthetic noisy datasets. The main technical challenge in Prestopping is determining the best stop point for its phase transition (we call it a best transition point). In Prestopping, a clean validation set or a known true noise rate is used for supervision, but they are usually hard to acquire in practice. In the last part, we introduce a novel self-transitional learning approach called MORPH, which automatically switches its learning phase at the best transition point without any supervision. Extensive experiments using five benchmark datasets demonstrate that only MORPH succeeds to construct a collection of almost true-labeled samples in a wide range of noise types. We leave the incorporation of SELFIE with MORPH as future work.</p>
    

    
    

    
    <div class="row">
      <div class="col-md-1"></div>
      <div class="col-md-10">
        <div class="row">
          <div class="col-12 col-md-3 pub-row-heading">Publication</div>
          <div class="col-12 col-md-9">Korea Advanced Institute of Science and Technology</div>
        </div>
      </div>
      <div class="col-md-1"></div>
    </div>
    <div class="d-md-none space-below"></div>
    

    <div class="space-below"></div>

    <div class="article-style"></div>

    








<div class="share-box" aria-hidden="true">
  <ul class="share">
    
      
      
      
        
      
      
      
      <li>
        <a href="https://twitter.com/intent/tweet?url=https://example.com/publication/2020disertation/&amp;text=Robust%20Learning%20under%20Label%20Noise%20with%20Deep%20Neural%20Networks%20%28PhD%20Dissertation%29" target="_blank" rel="noopener" class="share-btn-twitter">
          <i class="fab fa-twitter"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      <li>
        <a href="https://www.facebook.com/sharer.php?u=https://example.com/publication/2020disertation/&amp;t=Robust%20Learning%20under%20Label%20Noise%20with%20Deep%20Neural%20Networks%20%28PhD%20Dissertation%29" target="_blank" rel="noopener" class="share-btn-facebook">
          <i class="fab fa-facebook"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      <li>
        <a href="mailto:?subject=Robust%20Learning%20under%20Label%20Noise%20with%20Deep%20Neural%20Networks%20%28PhD%20Dissertation%29&amp;body=https://example.com/publication/2020disertation/" target="_blank" rel="noopener" class="share-btn-email">
          <i class="fas fa-envelope"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      <li>
        <a href="https://www.linkedin.com/shareArticle?url=https://example.com/publication/2020disertation/&amp;title=Robust%20Learning%20under%20Label%20Noise%20with%20Deep%20Neural%20Networks%20%28PhD%20Dissertation%29" target="_blank" rel="noopener" class="share-btn-linkedin">
          <i class="fab fa-linkedin-in"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      <li>
        <a href="whatsapp://send?text=Robust%20Learning%20under%20Label%20Noise%20with%20Deep%20Neural%20Networks%20%28PhD%20Dissertation%29%20https://example.com/publication/2020disertation/" target="_blank" rel="noopener" class="share-btn-whatsapp">
          <i class="fab fa-whatsapp"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      <li>
        <a href="https://service.weibo.com/share/share.php?url=https://example.com/publication/2020disertation/&amp;title=Robust%20Learning%20under%20Label%20Noise%20with%20Deep%20Neural%20Networks%20%28PhD%20Dissertation%29" target="_blank" rel="noopener" class="share-btn-weibo">
          <i class="fab fa-weibo"></i>
        </a>
      </li>
    
  </ul>
</div>











  
  
    




  














  
  





  </div>
</div>
  </div>

  <div class="page-footer">
    
    
    <div class="container">
      <footer class="site-footer">

  



  

  
  <p class="powered-by">
    
      <a href="/privacy/">Privacy Policy</a>
    
    
       &middot; 
      <a href="/terms/">Terms</a>
    
  </p>
  

  

  
  






  <p class="powered-by">
    
    
    
      
      
      
      
      
      
      Published with <a href="https://wowchemy.com/?utm_campaign=poweredby" target="_blank" rel="noopener">Wowchemy</a> — the free, <a href="https://github.com/wowchemy/wowchemy-hugo-modules" target="_blank" rel="noopener">open source</a> website builder that empowers creators.
    
  </p>
</footer>

    </div>
    
  </div>

  
<div id="modal" class="modal fade" role="dialog">
  <div class="modal-dialog">
    <div class="modal-content">
      <div class="modal-header">
        <h5 class="modal-title">Cite</h5>
        <button type="button" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body">
        <pre><code class="tex hljs"></code></pre>
      </div>
      <div class="modal-footer">
        <a class="btn btn-outline-primary my-1 js-copy-cite" href="#" target="_blank">
          <i class="fas fa-copy"></i> Copy
        </a>
        <a class="btn btn-outline-primary my-1 js-download-cite" href="#" target="_blank">
          <i class="fas fa-download"></i> Download
        </a>
        <div id="modal-error"></div>
      </div>
    </div>
  </div>
</div>

      

    
    <script src="/js/vendor-bundle.min.b73dfaac3b6499dc997741748a7c3fe2.js"></script>

    
    
    
      
      
        <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.imagesloaded/4.1.4/imagesloaded.pkgd.min.js" integrity="sha256-lqvxZrPLtfffUl2G/e7szqSvPBILGbwmsGE1MKlOi0Q=" crossorigin="anonymous"></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.isotope/3.0.6/isotope.pkgd.min.js" integrity="sha256-CBrpuqrMhXwcLLUd5tvQ4euBHCdh7wGlDfNz8vbu/iI=" crossorigin="anonymous"></script>
      

      
      

      

      
        
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.2.0/highlight.min.js" integrity="sha512-TDKKr+IvoqZnPzc3l35hdjpHD0m+b2EC2SrLEgKDRWpxf2rFCxemkgvJ5kfU48ip+Y+m2XVKyOCD85ybtlZDmw==" crossorigin="anonymous"></script>
        
        
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.2.0/languages/r.min.js" crossorigin="anonymous"></script>
        
      

    

    
    
    

    
    

    
    
    
      
      <script id="search-hit-fuse-template" type="text/x-template">
        <div class="search-hit" id="summary-{{key}}">
          <div class="search-hit-content">
            <div class="search-hit-name">
              <a href="{{relpermalink}}">{{title}}</a>
              <div class="article-metadata search-hit-type">{{type}}</div>
              <p class="search-hit-description">{{snippet}}</p>
            </div>
          </div>
        </div>
      </script>
      
        <script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/3.2.1/fuse.min.js" integrity="sha256-VzgmKYmhsGNNN4Ph1kMW+BjoYJM2jV5i4IlFoeZA9XI=" crossorigin="anonymous"></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/jquery.mark.min.js" integrity="sha256-4HLtjeVgH0eIB3aZ9mLYF6E8oU5chNdjU6p6rrXpl9U=" crossorigin="anonymous"></script>
      
    

    
    

    
    
    
    

    
    
      
      
      
      
      
      
      
    

    
    
    
    
    
    
    
    
      
      
    
    
    <script src="/en/js/wowchemy.min.d68ecd57c0ec1f1f61d65fd568f1c3a0.js"></script>

    






</body>
</html>
