[{"authors":["admin"],"categories":null,"content":"I am a forth-year Ph.D candidate in Graduate School of Knowledge Service Engineering at KAIST. Currently, my advisor is Prof. Jae-Gil Lee, and I am a representative student in Data Mining Lab.\nI am going to work at Google Research (HQ, Mountain View) as a research intern under the supervision of two hosts, Eunyoung Kim and Ming-Hsuan Yang.\nMy general research interests lie in improving the performance of machine learning (ML) techniques under real-world scenarios. I am particularly interested in designing more advanced approaches to handle large-scale and noisy data, which are two main real-world challenges to hinder the practical use of ML approaches.\n","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"/authors/admin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/admin/","section":"authors","summary":"I am a forth-year Ph.D candidate in Graduate School of Knowledge Service Engineering at KAIST. Currently, my advisor is Prof. Jae-Gil Lee, and I am a representative student in Data Mining Lab.\nI am going to work at Google Research (HQ, Mountain View) as a research intern under the supervision of two hosts, Eunyoung Kim and Ming-Hsuan Yang.\nMy general research interests lie in improving the performance of machine learning (ML) techniques under real-world scenarios.","tags":null,"title":"Hwanjun Song","type":"authors"},{"authors":["Sundong Kim","Hwanjun Song","Sejin Kim","Beomyoung Kim","Jae-Gil Lee"],"categories":null,"content":"","date":1579478400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1579478400,"objectID":"b7b895e292892ada50a248e554dbfa19","permalink":"/publication/2020pakdd_revisit/","publishdate":"2020-01-20T00:00:00Z","relpermalink":"/publication/2020pakdd_revisit/","section":"publication","summary":"In this manuscript, we introduce SurvRev, a next-generation revisit prediction model that can be tested directly in the business. The SurvRev model has many advantages. First, SurvRev can use partial observations which were considered as missing data and removed in the previous regression framework. By using deep survival analysis, we are able to estimate the next customer arrival from unknown distribution. Second, SurvRev is an event rate prediction model. It generates the predicted event rate of the next k days rather than predicting revisit interval and revisit intention directly. We showed the superiority of the SurvRev model by comparing with diverse baselines including the feature engineering model and the state-of-the-art deep survival models.","tags":null,"title":"Revisit Prediction by Deep Survival Analysis (PAKDD 2020, To appear)","type":"publication"},{"authors":["Dongmin Park","Hwanjun Song","Minseok Kim","Jae-Gil Lee"],"categories":null,"content":"","date":1577836800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1577836800,"objectID":"d7b4520c387fdf531c0fc45e72e1fbd9","permalink":"/publication/2020thewebconf_trap/","publishdate":"2020-01-01T00:00:00Z","relpermalink":"/publication/2020thewebconf_trap/","section":"publication","summary":"Finding low-dimensional embeddings of sparse high-dimensional data objects is important in many applications such as recommendation, graph mining, and natural language processing (NLP). Recently, autoencoder (AE)-based embedding approaches have achieved state-of-the-art performance in many tasks, especially in top-k recommendation tasks with user embedding or node classification tasks with node embedding. However, we find that many real-world data follow the power-law distribution with respect to the data object sparsity. When learning AE-based embeddings of these data, dense inputs move away from sparse inputs in an embedding space even when they are highly correlated. Resultingly, the embedding is distorted, which we call the polarization problem. In this paper, we propose TRAP that leverages two-level regularizers to effectively alleviate this problem. (i) The macroscopic regularizer adds a regularization term in the loss function to generally prevent dense input objects from being distant from other sparse input objects. (ii) The microscopic regularizer introduces a new object-wise parameter to individually entice each object to correlated neighbor objects rather than uncorrelated ones. Importantly, TRAP is a meta-algorithm that can be easily coupled with existing AE-based embedding methods with a simple modification. In extensive experiments on two representative embedding tasks using six-real world datasets, TRAP boosted the performance of the state-of-the-art algorithms by up to 31.53% and 94.99% respectively.","tags":null,"title":"TRAP: Two-level Regularized Autoencoder-based Embedding for Power-law Distributed Data (TheWebConf 2020, To appear)","type":"publication"},{"authors":["Hwanjun Song","Minseok Kim","Dongmin Park","Jae-Gil Lee"],"categories":null,"content":"","date":1574124000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1574124000,"objectID":"d971fa0f49ba6aefb56c61abb07b67e1","permalink":"/publication/2019arxiv_prestopping/","publishdate":"2019-11-19T00:40:00Z","relpermalink":"/publication/2019arxiv_prestopping/","section":"publication","summary":"Noisy labels are very common in real-world training data, which lead to poor generalization on test data because of overfitting to the noisy labels. In this paper, we claim that such overfitting can be avoided by “early stopping” training a deep neural network before the noisy labels are severely memorized. Then, we resume training the early stopped network using a “maximal safe set,” which maintains a collection of almost certainly true-labeled samples at each epoch since the early stop point. Putting them all together, our novel two-phase training method, called Prestopping, realizes noise-free training under any type of label noise for practical use. Extensive experiments using four image benchmark data sets verify that our method significantly outperforms four state-of-the-art methods in test error by 0.4–8.2 percent points under existence of real-world noise.","tags":null,"title":"Prestopping: How Does Early Stopping Help Generalization against Label Noise? (Arxiv 2019)","type":"publication"},{"authors":["Hwanjun Song","Minseok Kim","Sundong Kim","Jae-Gil Lee"],"categories":null,"content":"","date":1574121600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1574121600,"objectID":"1180f6bfd7adf4245ea5c5763f27cff4","permalink":"/publication/2019arxiv_carpediem/","publishdate":"2019-11-19T00:00:00Z","relpermalink":"/publication/2019arxiv_carpediem/","section":"publication","summary":"The performance of deep neural networks is significantly affected by how well mini-batches are constructed. In this paper, we propose a novel adaptive batch selection algorithm called Recency Bias that exploits the uncertain samples predicted inconsistently in recent iterations. The historical label predictions of each sample are used to evaluate its predictive uncertainty within a sliding window. By taking advantage of this design, Recency Bias not only accelerates the training step but also achieves a more accurate network. We demonstrate the superiority of Recency Bias by extensive evaluation on two independent tasks. Compared with existing batch selection methods, the results showed that Recency Bias reduced the test error by up to 20.5% in a fixed wall-clock training time. At the same time, it improved the training time by up to 59.3% to reach the same test error","tags":null,"title":"Carpe Diem, Seize the Samples Uncertain \"At the Moment\" for Adaptive Batch Selection (Arxiv 2019)","type":"publication"},{"authors":["Hwanjun Song","Minseok Kim","Jae-Gil Lee"],"categories":null,"content":"","date":1560556800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1560556800,"objectID":"0c81d0a89eae749812835a2035017a04","permalink":"/publication/2019icml_selfie/","publishdate":"2019-06-15T00:00:00Z","relpermalink":"/publication/2019icml_selfie/","section":"publication","summary":"Owing to the extremely high expressive power of deep neural networks, their side effect is to totally memorize training data even when the labels are extremely noisy. To overcome overfitting on the noisy labels, we propose a novel robust training method called SELFIE. Our key idea is to selectively refurbish and exploit unclean samples that can be corrected with high precision, thereby gradually increasing the number of available training samples. Taking advantage of this design, SELFIE effectively prevents the risk of noise accumulation from the false correction and fully exploits the training data. To validate the superiority of SELFIE, we conducted extensive experimentation using four real-world or synthetic data sets. The result showed that SELFIE remarkably improved absolute test error compared with two state-of-the-art methods.","tags":null,"title":"SELFIE: Refurbishing Unclean Samples for Robust Deep Learning (ICML 2019)","type":"publication"},{"authors":["Hwanjun Song","Jae-Gil Lee"],"categories":null,"content":"","date":1528588800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1528588800,"objectID":"d079f0af428af258ee6005bf7fcc8852","permalink":"/publication/2018sigmod_rpdbscan/","publishdate":"2018-06-10T00:00:00Z","relpermalink":"/publication/2018sigmod_rpdbscan/","section":"publication","summary":"In most parallel DBSCAN algorithms, neighboring points are assigned to the same data partition for parallel processing to facilitate calculation of the density of the neighbors. This data partitioning scheme causes a few critical problems including load imbalance between data partitions, especially in a skewed data set. To remedy these problems, we propose a cell-based data partitioning scheme, pseudo random partitioning , that randomly distributes small cells rather than the points themselves. It achieves high load balance regardless of data skewness while retaining the data contiguity required for DBSCAN. In addition, we build and broadcast a highly compact summary of the entire data set, which we call a two-level cell dictionary , to supplement random partitions. Then, we develop a novel parallel DBSCAN algorithm, Random Partitioning-DBSCAN (shortly, RP-DBSCAN), that uses pseudo random partitioning together with a two-level cell dictionary. The algorithm simultaneously finds the local clusters to each data partition and then merges these local clusters to obtain global clustering. To validate the merit of our approach, we implement RP-DBSCAN on Spark and conduct extensive experiments using various real-world data sets on 12 Microsoft Azure machines (48 cores). In RP-DBSCAN, data partitioning and cluster merging are very light, and clustering on each split is not dragged out by a specific worker. Therefore, the performance results show that RP-DBSCAN significantly outperforms the state-of-the-art algorithms by up to 180 times.","tags":null,"title":"RP-DBSCAN: A Superfast Parallel DBSCAN Algorithm based on Random Partitioning (SIGMOD 2018)","type":"publication"},{"authors":["Hwanjun Song","Jae-Gil Lee","Wook-Shin Han"],"categories":null,"content":"","date":1502582400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1502582400,"objectID":"f5b65c3c512c9434b689193e59f7644b","permalink":"/publication/2017kdd_pamae/","publishdate":"2017-08-13T00:00:00Z","relpermalink":"/publication/2017kdd_pamae/","section":"publication","summary":"The k-medoids algorithm is one of the best-known clustering algorithms. Despite this, however, it is not as widely used for big data analytics as the k-means algorithm, mainly because of its high computational complexity. Many studies have attempted to solve the efficiency problem of the k-medoids algorithm, but all such studies have improved efficiency at the expense of accuracy. In this paper, we propose a novel parallel k-medoids algorithm, which we call PAMAE, that achieves both high accuracy and high efficiency. We identify two factors \\\"global search\\\" and \\\"entire data\\\" that are essential to achieving high accuracy, but are also very time-consuming if considered simultaneously. Thus, our key idea is to apply them individually through two phases, parallel seeding and parallel refinement, neither of which is costly. The first phase performs global search over sampled data, and the second phase performs local search over entire data. Our theoretical analysis proves that this serial execution of the two phases leads to an accurate solution that would be achieved by global search over entire data. In order to validate the merit of our approach, we implement PAMAE on Spark as well as Hadoop and conduct extensive experiments using various real-world data sets on 12 Microsoft Azure machines (48 cores). The results show that PAMAE significantly outperforms most of recent parallel algorithms and, at the same time, produces a clustering quality as comparable as the previous most-accurate algorithm.","tags":null,"title":"PAMAE: Parallel k-Medoids Clustering with High aAccuracy and Efficiency (KDD 2017)","type":"publication"}]